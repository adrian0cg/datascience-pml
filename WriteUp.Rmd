---
title: "PML-Course Project"
author: "Adrian CG"
date: "22 Feb 2015"
output:
  html_document:
    keep_md: yes
    theme: journal
bibliography: bibliography.bib
---

```{r downloadData, echo=FALSE, results='hide', cache=FALSE}
library(knitcitations); cleanbib(); bib <- read.bibtex("bibliography.bib");
library(caret)
library(randomForest)
source("utils.R")
pmlTraining <- read.csv("training.csv")
pmlTesting <- read.csv("testing.csv")
```

## Data Preprocessing
On the first glance into the data, we observe a lot of wrongly concluded data types for the scalar measurement columns (ranging from 8 to 159). Thus, we coerce them to the appropriate numeric classes:

```{r correctClasses, cache=TRUE}
for (i in 8:159) {
    pmlTraining[,i] <- as.numeric(pmlTraining[,i])
    pmlTesting[,i] <- as.numeric(pmlTesting[,i])
}
str(pmlTraining[,c(1:8,159,160)])
```


## Prediction
### Data-Splitting
To asses our model we split our original training data in to 60% real training and 40% into a validation set, which we we later use for model assessment.
```{r createDataPartition, echo=TRUE, cache=TRUE}
inValidation <- createDataPartition(pmlTraining$classe, p=0.4, list=FALSE)
```

### Model Building
#### Feature Selection
After manual inspection of the entries in the training set, we found some samples at the end of each execrise window containing a full set of the derived variables, while others (beginning and within windows) only contain the base measurements. For classification purposes of a whole movement as mentioned in the original paper `r citep(bib[["Velloso2013"]])`, the derived variables seem to be a better fit. However, sine they are rather sparse and can't be recomputed for single points within the exercise repetition, we constrain ourselves to the variables that don't contain `NA`s.

```{r restrictMeasured, echo=TRUE, cache=TRUE}
measuredColumns <- !apply(is.na(pmlTesting), 2, any)
```

To check the existing data for some zero variance abnomalities, that could also negatively influence the prediction, we use the zero variance checks of the `caret` package and will subsequently remove them.
```{r removeNZV, echo=TRUE, cache=TRUE}
nzvPml <- nearZeroVar(pmlTraining, saveMetrics= TRUE)
```

Likewise, for certain prediction algorithms, which are not model bound, it would be unwise to leave predictors in the dataset, that have unique values for most entries like the `X`, `new_window`, `num_window`, `raw_timestamp_part_1` that could lead to overfitting. Also `cvtd_timestamp` is somewhat synonymous to `user_name`. Thus we retain as simplified dataset:

```{r removeOverfitters, echo=TRUE, cache=TRUE}
removeOverfitters <- rep(TRUE,160); removeOverfitters[c(1,3,5,7)] = FALSE;
pmlTrain <- pmlTraining[-inValidation, !(nzvPml$zeroVar | nzvPml$nzv) & measuredColumns & removeOverfitters]
pmlEval <- pmlTraining[inValidation, !(nzvPml$zeroVar | nzvPml$nzv) & measuredColumns & removeOverfitters]
pmlTest <- pmlTesting[, !(nzvPml$zeroVar | nzvPml$nzv) & measuredColumns & removeOverfitters]
```

### Algorithm Selection
We have been trying to get a successful computation run using the `caret` package and any of the many models available therein, however all tried algorithms (using the parallel computation routines available) ended up consuming all available ressources (some CPU-bound, some Memory-Bound) of our Late 2013 MacBook Pro (4 cores, 16GB). Thus, we resolved to a "simpler" model using random forests from the `randomForest` package.

### Computing Model & Assessing Prediction Accuracy
Since `randomForest` allows for computation of the model combined with an assessment of the prediction accuracy for a "test set" (here it will be our `pmlEval` set) in one pass, we compute that jointly:
```{r computeRandomForest, cache=TRUE}
regressors <- colnames(pmlTrain)!= "classe";
halRF <- randomForest(
    x=pmlTrain[,regressors], y=pmlTrain[,"classe"],
    xtest=pmlEval[,regressors], ytest=pmlEval[,"classe"],
    importance=TRUE, proximity=TRUE, keep.forest=TRUE
);
print(halRF)
```

### Application on Original Test-Dataset

## References
```{r bibiliography, echo=FALSE, cache=TRUE, message=FALSE, results='asis'}
print(bibliography(), "html")
```

## Appendix
### Raw Usable Parameter List
```{r usableParameters, echo=FALSE, cache=TRUE}
str(names(pmlTrain),vec.len=55)
```

